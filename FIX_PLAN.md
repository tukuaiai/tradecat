# TradeCat é¡¹ç›®ä¿®å¤æ–¹æ¡ˆ

> ç”Ÿæˆæ—¶é—´: 2025-01-03
> ç‰ˆæœ¬: v1.0
> çŠ¶æ€: å¾…æ‰§è¡Œ

---

## ğŸ“‹ ç›®å½•

- [P0 - é«˜ä¼˜å…ˆçº§é—®é¢˜](#p0---é«˜ä¼˜å…ˆçº§é—®é¢˜)
- [P1 - ä¸­ä¼˜å…ˆçº§é—®é¢˜](#p1---ä¸­ä¼˜å…ˆçº§é—®é¢˜)
- [P2 - ä½ä¼˜å…ˆçº§ä¼˜åŒ–](#p2---ä½ä¼˜å…ˆçº§ä¼˜åŒ–)
- [æ¶æ„é‡æ„å»ºè®®](#æ¶æ„é‡æ„å»ºè®®)
- [æ‰§è¡Œè®¡åˆ’](#æ‰§è¡Œè®¡åˆ’)

---

## P0 - é«˜ä¼˜å…ˆçº§é—®é¢˜

### é—®é¢˜ 1: ç¡¬ç¼–ç æ•°æ®åº“è·¯å¾„

**é—®é¢˜ ID**: P0-001
**ä¸¥é‡ç¨‹åº¦**: ğŸ”´ é«˜
**å½±å“èŒƒå›´**: éƒ¨ç½²/è¿ç§»
**ä¿®å¤éš¾åº¦**: ä½

#### é—®é¢˜æè¿°

`services/trading-service/config/.env.example` ä¸­ç¡¬ç¼–ç äº†ç»å¯¹è·¯å¾„ï¼Œå¯¼è‡´é¡¹ç›®è¿ç§»æˆ–éƒ¨ç½²æ—¶å¿…é¡»æ‰‹åŠ¨ä¿®æ”¹ã€‚

```python
# å½“å‰é…ç½®
INDICATOR_SQLITE_PATH=/home/lenovo/.projects/tradecat/libs/database/services/telegram-service/market_data.db
```

#### ä¿®å¤æ–¹æ¡ˆ

**æ­¥éª¤ 1**: ä¿®æ”¹ `services/trading-service/config/.env.example`

```diff
- INDICATOR_SQLITE_PATH=/home/lenovo/.projects/tradecat/libs/database/services/telegram-service/market_data.db
+ INDICATOR_SQLITE_PATH=${PROJECT_ROOT}/libs/database/services/telegram-service/market_data.db
```

**æ­¥éª¤ 2**: ä¿®æ”¹ `services/trading-service/src/simple_scheduler.py`

```python
# ç¬¬ 29 è¡Œé™„è¿‘
PROJECT_ROOT = os.path.dirname(os.path.dirname(TRADING_SERVICE_DIR))
SQLITE_PATH = os.environ.get(
    "INDICATOR_SQLITE_PATH",
    os.path.join(PROJECT_ROOT, "libs/database/services/telegram-service/market_data.db")
).replace("${PROJECT_ROOT}", PROJECT_ROOT)
```

**æ­¥éª¤ 3**: éªŒè¯

```bash
# æµ‹è¯•é…ç½®è§£æ
cd services/trading-service
python3 -c "
import os
os.environ['PROJECT_ROOT'] = '/tmp/tradecat'
path = '\${PROJECT_ROOT}/libs/db/market_data.db'
print(path.replace('\${PROJECT_ROOT}', os.environ['PROJECT_ROOT']))
"
```

#### é¢„æœŸæ•ˆæœ

- é¡¹ç›®å¯è¿ç§»åˆ°ä»»æ„è·¯å¾„
- æ”¯æŒå®¹å™¨åŒ–éƒ¨ç½²ï¼ˆDocker/K8sï¼‰

---

### é—®é¢˜ 2: ä¾èµ–ç‰ˆæœ¬æœªé”å®š

**é—®é¢˜ ID**: P0-002
**ä¸¥é‡ç¨‹åº¦**: ğŸ”´ é«˜
**å½±å“èŒƒå›´**: ç”Ÿäº§ç¯å¢ƒç¨³å®šæ€§
**ä¿®å¤éš¾åº¦**: ä¸­

#### é—®é¢˜æè¿°

æ‰€æœ‰ `requirements.txt` ä½¿ç”¨ `>=` ç‰ˆæœ¬å·ï¼Œå¯èƒ½å¯¼è‡´ä¾èµ–æ¼‚ç§»ã€‚

#### ä¿®å¤æ–¹æ¡ˆ

**æ­¥éª¤ 1**: ç”Ÿæˆé”æ–‡ä»¶

```bash
# ä¸ºæ¯ä¸ªæœåŠ¡ç”Ÿæˆ requirements.lock.txt
cd services/data-service
pip freeze > requirements.lock.txt

cd ../trading-service
pip freeze > requirements.lock.txt

cd ../telegram-service
pip freeze > requirements.lock.txt

cd ../order-service
pip freeze > requirements.lock.txt
```

**æ­¥éª¤ 2**: æ‰‹åŠ¨é”å®šå…³é”®ä¾èµ–

ç¼–è¾‘å„æœåŠ¡çš„ `requirements.txt`:

```txt
# data-service/requirements.txt
psycopg[binary,pool]==3.1.18
aiohttp==3.9.3
ccxt==4.2.0
requests==2.31.0
cryptofeed==2.4.0

# trading-service/requirements.txt
psycopg[binary,pool]==3.1.18
pandas==2.2.0
numpy==1.26.4
TA-Lib==0.4.28
m-patternpy==2.0.0

# telegram-service/requirements.txt
python-telegram-bot==20.7
aiohttp==3.9.3
httpx==0.25.2
requests==2.31.0
```

**æ­¥éª¤ 3**: æ›´æ–° `scripts/init.sh`

```bash
#!/bin/bash
# æ·»åŠ ä¾èµ–é”å®šæ£€æŸ¥
init_service() {
    local service=$1
    if [ -f "services/$service/requirements.lock.txt" ]; then
        pip install -r services/$service/requirements.lock.txt
    else
        pip install -r services/$service/requirements.txt
    fi
}
```

#### é¢„æœŸæ•ˆæœ

- ç¡®ä¿ç”Ÿäº§ç¯å¢ƒä¾èµ–ç‰ˆæœ¬ä¸€è‡´
- é¿å…å› ä¾èµ–æ›´æ–°å¯¼è‡´çš„ç ´åæ€§å˜æ›´

---

### é—®é¢˜ 3: ç¯å¢ƒå˜é‡å‘½åä¸ä¸€è‡´

**é—®é¢˜ ID**: P0-003
**ä¸¥é‡ç¨‹åº¦**: ğŸŸ¡ ä¸­
**å½±å“èŒƒå›´**: ä»£ç ç»´æŠ¤
**ä¿®å¤éš¾åº¦**: ä½

#### é—®é¢˜æè¿°

`BOT_TOKEN` å’Œ `TELEGRAM_BOT_TOKEN` æ··ç”¨ï¼Œå®¹æ˜“æ··æ·†ã€‚

#### ä¿®å¤æ–¹æ¡ˆ

**æ­¥éª¤ 1**: ç»Ÿä¸€å˜é‡åä¸º `TELEGRAM_BOT_TOKEN`

ä¿®æ”¹ `services/telegram-service/config/.env.example`:

```diff
- BOT_TOKEN=your_bot_token_here
+ TELEGRAM_BOT_TOKEN=your_bot_token_here
```

**æ­¥éª¤ 2**: ä¿®æ”¹ `services/telegram-service/src/bot/app.py`

```python
# ç¬¬ 259 è¡Œé™„è¿‘
- BOT_TOKEN = _require_env('BOT_TOKEN', required=True)
- TELEGRAM_BOT_TOKEN = BOT_TOKEN  # ä¸ºäº†å…¼å®¹æ€§æ·»åŠ åˆ«å
+ TELEGRAM_BOT_TOKEN = _require_env('TELEGRAM_BOT_TOKEN', required=True)

# ç§»é™¤æ‰€æœ‰ BOT_TOKEN å¼•ç”¨
```

**æ­¥éª¤ 3**: æ›´æ–°æ–‡æ¡£

ä¿®æ”¹ `README.md` ä¸­æ‰€æœ‰ `BOT_TOKEN` ä¸º `TELEGRAM_BOT_TOKEN`

#### é¢„æœŸæ•ˆæœ

- å˜é‡å‘½åç»Ÿä¸€
- å‡å°‘é…ç½®é”™è¯¯

---

### é—®é¢˜ 4: æ•°æ®åº“æŸ¥è¯¢ç¼ºå°‘ç´¢å¼•ä¼˜åŒ–

**é—®é¢˜ ID**: P0-004
**ä¸¥é‡ç¨‹åº¦**: ğŸ”´ é«˜
**å½±å“èŒƒå›´**: æ€§èƒ½
**ä¿®å¤éš¾åº¦**: ä¸­

#### é—®é¢˜æè¿°

`simple_scheduler.py` ä¸­çš„ä¼˜å…ˆçº§æŸ¥è¯¢åœ¨å¤§æ•°æ®é‡æ—¶æ€§èƒ½å·®ã€‚

#### ä¿®å¤æ–¹æ¡ˆ

**æ­¥éª¤ 1**: åˆ›å»ºç´¢å¼•æ–‡ä»¶

æ–°å»º `libs/database/db/schema/008_optimize_priority_queries.sql`:

```sql
-- ä¸º K çº¿ä¼˜å…ˆçº§æŸ¥è¯¢ä¼˜åŒ–
CREATE INDEX IF NOT EXISTS idx_candles_5m_symbol_ts
ON market_data.candles_5m(symbol, bucket_ts DESC);

CREATE INDEX IF NOT EXISTS idx_candles_5m_ts
ON market_data.candles_5m(bucket_ts DESC);

-- ä¸ºæœŸè´§ä¼˜å…ˆçº§æŸ¥è¯¢ä¼˜åŒ–
CREATE INDEX IF NOT EXISTS idx_futures_metrics_5m_symbol_ts
ON market_data.binance_futures_metrics_5m(symbol, create_time DESC);

-- åˆ›å»ºæŒç»­èšåˆè§†å›¾ï¼ˆæ¯å°æ—¶æ›´æ–°ï¼‰
CREATE MATERIALIZED VIEW IF NOT EXISTS market_data.candles_5m_1h_agg
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 hour', bucket_ts) AS hour,
    symbol,
    SUM(quote_volume) as total_qv,
    AVG((high-low)/NULLIF(close,0)) as volatility,
    FIRST(close, bucket_ts) as open,
    LAST(close, bucket_ts) as close
FROM market_data.candles_5m
GROUP BY hour, symbol;

-- è®¾ç½®åˆ·æ–°ç­–ç•¥
SELECT add_continuous_aggregate_policy('market_data.candles_5m_1h_agg',
    start_offset => INTERVAL '1 hour',
    end_offset => INTERVAL '5 minutes',
    schedule_interval => INTERVAL '1 hour');
```

**æ­¥éª¤ 2**: åº”ç”¨ç´¢å¼•

```bash
PGPASSWORD=postgres psql -h localhost -p 5433 -U postgres -d market_data -f \
    libs/database/db/schema/008_optimize_priority_queries.sql
```

**æ­¥éª¤ 3**: ä¼˜åŒ–æŸ¥è¯¢é€»è¾‘

ä¿®æ”¹ `services/trading-service/src/simple_scheduler.py`:

```python
def _query_kline_priority(top_n: int = 30) -> set:
    """Kçº¿ç»´åº¦ä¼˜å…ˆçº§ - ä½¿ç”¨é¢„èšåˆè§†å›¾"""
    symbols = set()
    try:
        with psycopg.connect(DB_URL) as conn:
            # ä½¿ç”¨ 1 å°æ—¶èšåˆè§†å›¾æ›¿ä»£åŸå§‹æŸ¥è¯¢
            sql = """
                WITH base AS (
                    SELECT symbol,
                           SUM(total_qv) as volume_24h,
                           AVG(volatility) as avg_volatility
                    FROM market_data.candles_5m_1h_agg
                    WHERE hour > NOW() - INTERVAL '24 hours'
                    GROUP BY symbol
                ),
                ranks AS (
                    SELECT symbol,
                           ROW_NUMBER() OVER (ORDER BY volume_24h DESC) as v_rank,
                           ROW_NUMBER() OVER (ORDER BY avg_volatility DESC) as vol_rank
                    FROM base
                )
                SELECT DISTINCT symbol FROM ranks
                WHERE v_rank <= %s OR vol_rank <= %s
            """
            cur = conn.execute(sql, (top_n, top_n))
            symbols.update(r[0] for r in cur.fetchall())
    except Exception as e:
        log(f"Kçº¿ä¼˜å…ˆçº§æŸ¥è¯¢å¤±è´¥: {e}")
    return symbols
```

#### é¢„æœŸæ•ˆæœ

- æŸ¥è¯¢é€Ÿåº¦æå‡ 5-10 å€
- é™ä½æ•°æ®åº“è´Ÿè½½

---

### é—®é¢˜ 5: SQLite å¹¶å‘å†™å…¥é£é™©

**é—®é¢˜ ID**: P0-005
**ä¸¥é‡ç¨‹åº¦**: ğŸ”´ é«˜
**å½±å“èŒƒå›´**: æ•°æ®å®Œæ•´æ€§
**ä¿®å¤éš¾åº¦**: ä¸­

#### é—®é¢˜æè¿°

å¤šçº¿ç¨‹åŒæ—¶å†™å…¥ SQLite å¯èƒ½å¯¼è‡´ "database is locked"ã€‚

#### ä¿®å¤æ–¹æ¡ˆ

**æ­¥éª¤ 1**: å¯ç”¨ WAL æ¨¡å¼

ä¿®æ”¹ `services/trading-service/src/simple_scheduler.py`:

```python
import sqlite3
import threading

# å…¨å±€è¿æ¥é”
_sqlite_lock = threading.Lock()

def get_sqlite_connection() -> sqlite3.Connection:
    """è·å– SQLite è¿æ¥ï¼Œå¯ç”¨ WAL æ¨¡å¼"""
    conn = sqlite3.connect(SQLITE_PATH, check_same_thread=False)
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")
    conn.execute("PRAGMA busy_timeout=5000")  # 5ç§’è¶…æ—¶
    return conn

def get_indicator_latest(interval: str) -> datetime:
    """æŸ¥è¯¢ SQLite æŒ‡æ ‡è¯¥å‘¨æœŸæœ€æ–°æ•°æ®æ—¶é—´"""
    try:
        with _sqlite_lock:  # åŠ é”
            conn = get_sqlite_connection()
            row = conn.execute("""
                SELECT MAX(æ•°æ®æ—¶é—´) as latest FROM [MACDæŸ±çŠ¶æ‰«æå™¨.py] WHERE å‘¨æœŸ = ?
            """, (interval,)).fetchone()
            conn.close()
            if row and row[0]:
                ts_str = row[0].replace("+00:00", "").replace("T", " ")
                return datetime.fromisoformat(ts_str).replace(tzinfo=timezone.utc)
            return None
    except Exception as e:
        log(f"SQLite æŸ¥è¯¢å¤±è´¥: {e}")
        return None
```

**æ­¥éª¤ 2**: æ‰¹é‡å†™å…¥ä¼˜åŒ–

åˆ›å»ºæ–°æ–‡ä»¶ `services/trading-service/src/utils/sqlite_pool.py`:

```python
"""SQLite è¿æ¥æ± """
import sqlite3
import threading
from contextlib import contextmanager
from typing import Optional

class SQLitePool:
    """SQLite è¿æ¥æ± """

    def __init__(self, db_path: str, pool_size: int = 3):
        self.db_path = db_path
        self.pool_size = pool_size
        self._pool = []
        self._lock = threading.Lock()
        self._initialize_pool()

    def _initialize_pool(self):
        """åˆå§‹åŒ–è¿æ¥æ± """
        for _ in range(self.pool_size):
            conn = self._create_connection()
            if conn:
                self._pool.append(conn)

    def _create_connection(self) -> Optional[sqlite3.Connection]:
        """åˆ›å»ºæ–°è¿æ¥"""
        try:
            conn = sqlite3.connect(
                self.db_path,
                check_same_thread=False
            )
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute("PRAGMA synchronous=NORMAL")
            conn.execute("PRAGMA busy_timeout=5000")
            return conn
        except Exception as e:
            return None

    @contextmanager
    def get_connection(self):
        """è·å–è¿æ¥"""
        conn = None
        try:
            with self._lock:
                if self._pool:
                    conn = self._pool.pop()
            if not conn:
                conn = self._create_connection()
            yield conn
        finally:
            if conn:
                with self._lock:
                    self._pool.append(conn)

    def close_all(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        with self._lock:
            for conn in self._pool:
                try:
                    conn.close()
                except:
                    pass
            self._pool.clear()


# å…¨å±€è¿æ¥æ± å®ä¾‹
_sqlite_pool: Optional[SQLitePool] = None
_pool_lock = threading.Lock()

def get_sqlite_pool(db_path: str) -> SQLitePool:
    """è·å–å…¨å±€è¿æ¥æ± """
    global _sqlite_pool
    with _pool_lock:
        if _sqlite_pool is None:
            _sqlite_pool = SQLitePool(db_path, pool_size=3)
        return _sqlite_pool
```

**æ­¥éª¤ 3**: æ›´æ–°æŒ‡æ ‡å†™å…¥é€»è¾‘

ä¿®æ”¹ `services/trading-service/src/indicators/base.py`:

```python
from utils.sqlite_pool import get_sqlite_pool

def batch_insert(table_name: str, data: list):
    """æ‰¹é‡æ’å…¥æ•°æ®"""
    pool = get_sqlite_pool(SQLITE_PATH)
    with pool.get_connection() as conn:
        cursor = conn.cursor()
        cursor.executemany(
            f"INSERT OR REPLACE INTO [{table_name}] VALUES ({','.join(['?'] * len(data[0]))})",
            data
        )
        conn.commit()
```

#### é¢„æœŸæ•ˆæœ

- æ¶ˆé™¤æ•°æ®åº“é”å†²çª
- æå‡å†™å…¥ååé‡ 3-5 å€

---

## P1 - ä¸­ä¼˜å…ˆçº§é—®é¢˜

### é—®é¢˜ 6: é…ç½®åˆ†æ•£ç®¡ç†

**é—®é¢˜ ID**: P1-001
**ä¸¥é‡ç¨‹åº¦**: ğŸŸ¡ ä¸­
**å½±å“èŒƒå›´**: å¯ç»´æŠ¤æ€§
**ä¿®å¤éš¾åº¦**: é«˜

#### ä¿®å¤æ–¹æ¡ˆ

**æ­¥éª¤ 1**: åˆ›å»ºç»Ÿä¸€é…ç½®åŠ è½½å™¨

æ–°å»º `libs/common/config_loader.py`:

```python
"""ç»Ÿä¸€é…ç½®åŠ è½½å™¨"""
import os
import json
from pathlib import Path
from typing import Any, Optional, Dict
from dotenv import load_dotenv

class ConfigLoader:
    """é…ç½®åŠ è½½å™¨"""

    def __init__(self, project_root: Path, service_name: str):
        self.project_root = project_root
        self.service_name = service_name
        self._cache: Dict[str, Any] = {}

        # åŠ è½½é¡ºåº: å…¬å…±é…ç½® -> æœåŠ¡é…ç½® -> ç¯å¢ƒå˜é‡
        self._load_configs()

    def _load_configs(self):
        """æŒ‰ä¼˜å…ˆçº§åŠ è½½é…ç½®"""
        # 1. åŠ è½½å…¬å…±é…ç½®
        common_env = self.project_root / "config" / ".env"
        if common_env.exists():
            load_dotenv(common_env, override=False)

        # 2. åŠ è½½æœåŠ¡ç§æœ‰é…ç½®
        service_env = (
            self.project_root /
            "services" /
            self.service_name /
            "config" /
            ".env"
        )
        if service_env.exists():
            load_dotenv(service_env, override=True)

    def get(self, key: str, default: Any = None, required: bool = False) -> Any:
        """è·å–é…ç½®é¡¹"""
        if key in self._cache:
            return self._cache[key]

        value = os.getenv(key, default)

        if required and value is None:
            raise ValueError(
                f"é…ç½®é¡¹ {key} æœªè®¾ç½®ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶"
            )

        self._cache[key] = value
        return value

    def get_bool(self, key: str, default: bool = False) -> bool:
        """è·å–å¸ƒå°”å€¼"""
        value = self.get(key, str(default))
        return value.lower() in ('true', '1', 'yes', 'on')

    def get_int(self, key: str, default: int = 0) -> int:
        """è·å–æ•´æ•°å€¼"""
        value = self.get(key, str(default))
        try:
            return int(value)
        except ValueError:
            raise ValueError(f"é…ç½®é¡¹ {key} å¿…é¡»æ˜¯æ•´æ•°: {value}")

    def get_float(self, key: str, default: float = 0.0) -> float:
        """è·å–æµ®ç‚¹æ•°"""
        value = self.get(key, str(default))
        try:
            return float(value)
        except ValueError:
            raise ValueError(f"é…ç½®é¡¹ {key} å¿…é¡»æ˜¯æµ®ç‚¹æ•°: {value}")

    def get_list(self, key: str, default: str = "", sep: str = ",") -> list:
        """è·å–åˆ—è¡¨"""
        value = self.get(key, default)
        return [item.strip() for item in value.split(sep) if item.strip()]


def get_config(project_root: Path, service_name: str) -> ConfigLoader:
    """è·å–é…ç½®å®ä¾‹"""
    return ConfigLoader(project_root, service_name)
```

**æ­¥éª¤ 2**: æ›´æ–°å„æœåŠ¡ä½¿ç”¨ç»Ÿä¸€é…ç½®

ä¿®æ”¹ `services/trading-service/src/config.py`:

```python
from pathlib import Path
from libs.common.config_loader import get_config

PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
config = get_config(PROJECT_ROOT, "trading-service")

# ä½¿ç”¨é…ç½®
DATABASE_URL = config.get("DATABASE_URL", required=True)
MAX_WORKERS = config.get_int("MAX_WORKERS", default=4)
INTERVALS = config.get_list("INTERVALS", default="1m,5m,15m,1h")
```

#### é¢„æœŸæ•ˆæœ

- é…ç½®ç»Ÿä¸€ç®¡ç†
- å‡å°‘é‡å¤ä»£ç 

---

### é—®é¢˜ 7: è™šæ‹Ÿç¯å¢ƒé‡å¤ä¾èµ–

**é—®é¢˜ ID**: P1-002
**ä¸¥é‡ç¨‹åº¦**: ğŸŸ¡ ä¸­
**å½±å“èŒƒå›´**: èµ„æºå ç”¨
**ä¿®å¤éš¾åº¦**: é«˜

#### ä¿®å¤æ–¹æ¡ˆ

**æ­¥éª¤ 1**: è¯„ä¼°ä½¿ç”¨ Poetry

```bash
# å®‰è£… Poetry
pip install poetry

# è½¬æ¢é¡¹ç›®ç»“æ„
cd /home/lenovo/.projects/tradecat

# åˆ›å»ºæ ¹ç›®å½• pyproject.toml
cat > pyproject.toml << 'EOF'
[tool.poetry]
name = "tradecat"
version = "1.0.0"
description = "åŠ å¯†è´§å¸é‡åŒ–äº¤æ˜“æ•°æ®å¹³å°"
authors = ["tukuaiai"]

[tool.poetry.dependencies]
python = "^3.10"
psycopg = {extras = ["binary", "pool"], version = "^3.1.0"}
aiohttp = "^3.9.0"
ccxt = "^4.0.0"
requests = "^2.31.0"
pandas = "^2.0.0"
numpy = "^1.24.0"
python-telegram-bot = "^20.0"

[tool.poetry.group.dev.dependencies]
pytest = "^7.4.0"
ruff = "^0.1.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
EOF
```

**æ­¥éª¤ 2**: ä¸ºæ¯ä¸ªæœåŠ¡åˆ›å»ºç‹¬ç«‹é…ç½®

```bash
# data-service
cd services/data-service
poetry init --name=tradecat-data-service

# trading-service
cd ../trading-service
poetry init --name=tradecat-trading-service

# telegram-service
cd ../telegram-service
poetry init --name=tradecat-telegram-service
```

**æ­¥éª¤ 3**: å®‰è£…ä¾èµ–

```bash
# ä½¿ç”¨ uv æ›¿ä»£ Poetryï¼ˆæ›´å¿«ï¼‰
pip install uv

# å®‰è£…æ ¹ä¾èµ–
uv pip install -r requirements.txt

# å„æœåŠ¡ä½¿ç”¨æ ¹è™šæ‹Ÿç¯å¢ƒ
export VIRTUAL_ENV=/home/lenovo/.projects/tradecat/.venv
source $VIRTUAL_ENV/bin/activate
```

#### é¢„æœŸæ•ˆæœ

- å‡å°‘ç£ç›˜å ç”¨ 50%+
- åŠ å¿«ä¾èµ–å®‰è£…é€Ÿåº¦

---

### é—®é¢˜ 8: æ—¥å¿—é…ç½®åˆ†æ•£

**é—®é¢˜ ID**: P1-003
**ä¸¥é‡ç¨‹åº¦**: ğŸŸ¡ ä¸­
**å½±å“èŒƒå›´**: å¯è§‚æµ‹æ€§
**ä¿®å¤éš¾åº¦**: ä¸­

#### ä¿®å¤æ–¹æ¡ˆ

**æ­¥éª¤ 1**: åˆ›å»ºç»Ÿä¸€æ—¥å¿—é…ç½®

æ–°å»º `libs/common/logging_config.py`:

```python
"""ç»Ÿä¸€æ—¥å¿—é…ç½®"""
import logging
import sys
from pathlib import Path
from typing import Optional

def setup_logging(
    service_name: str,
    log_level: str = "INFO",
    log_dir: Optional[Path] = None,
):
    """é…ç½®æ—¥å¿—"""

    # åˆ›å»ºæ ¼å¼åŒ–å™¨
    formatter = logging.Formatter(
        fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # æ ¹æ—¥å¿—å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, log_level.upper()))

    # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
    root_logger.handlers.clear()

    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.DEBUG)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)

    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¯é€‰ï¼‰
    if log_dir:
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = log_dir / f"{service_name}.log"

        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)

    # ç¬¬ä¸‰æ–¹åº“æ—¥å¿—é™çº§
    for logger_name in ['httpx', 'apscheduler', 'urllib3']:
        logging.getLogger(logger_name).setLevel(logging.WARNING)

    return root_logger


def get_logger(name: str) -> logging.Logger:
    """è·å–æ—¥å¿—å™¨"""
    return logging.getLogger(name)
```

**æ­¥éª¤ 2**: æ›´æ–°å„æœåŠ¡

ä¿®æ”¹ `services/trading-service/src/simple_scheduler.py`:

```python
from libs.common.logging_config import setup_logging, get_logger

# åˆå§‹åŒ–æ—¥å¿—
setup_logging(
    service_name="trading-service",
    log_level=os.getenv("LOG_LEVEL", "INFO"),
    log_dir=Path(__file__).parent.parent / "logs"
)

logger = get_logger(__name__)

# æ›¿æ¢æ‰€æœ‰ print
- log(msg: str):
-     print(f"[{datetime.now():%Y-%m-%d %H:%M:%S}] {msg}", flush=True)
+ logger.info(msg)
```

#### é¢„æœŸæ•ˆæœ

- æ—¥å¿—æ ¼å¼ç»Ÿä¸€
- æ”¯æŒç»“æ„åŒ–æ—¥å¿—

---

### é—®é¢˜ 9: é”™è¯¯å¤„ç†è¿‡äºå®½æ³›

**é—®é¢˜ ID**: P1-004
**ä¸¥é‡ç¨‹åº¦**: ğŸŸ¡ ä¸­
**å½±å“èŒƒå›´**: å¯è°ƒè¯•æ€§
**ä¿®å¤éš¾åº¦**: ä½

#### ä¿®å¤æ–¹æ¡ˆ

**æ­¥éª¤ 1**: åˆ›å»ºé”™è¯¯å¤„ç†å·¥å…·

æ–°å»º `libs/common/error_handler.py`:

```python
"""é”™è¯¯å¤„ç†å·¥å…·"""
import logging
from functools import wraps
from typing import Callable, TypeVar, Optional
from psycopg import OperationalError, DatabaseError
from sqlite3 import OperationalError as SQLiteOperationalError

T = TypeVar('T')

logger = logging.getLogger(__name__)

def handle_database_errors(func: Callable[..., T]) -> Callable[..., T]:
    """æ•°æ®åº“é”™è¯¯å¤„ç†è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except OperationalError as e:
            logger.error(f"æ•°æ®åº“æ“ä½œé”™è¯¯: {e}", exc_info=True)
            raise
        except DatabaseError as e:
            logger.error(f"æ•°æ®åº“é”™è¯¯: {e}", exc_info=True)
            raise
        except SQLiteOperationalError as e:
            logger.error(f"SQLite é”™è¯¯: {e}", exc_info=True)
            raise
    return wrapper


def retry_on_failure(
    max_retries: int = 3,
    retryable_exceptions: tuple = (Exception,),
    backoff: float = 1.0,
):
    """å¤±è´¥é‡è¯•è£…é¥°å™¨"""
    def decorator(func: Callable[..., T]) -> Callable[..., T]:
        @wraps(func)
        def wrapper(*args, **kwargs):
            import time

            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except retryable_exceptions as e:
                    last_exception = e
                    if attempt < max_retries - 1:
                        wait_time = backoff * (2 ** attempt)
                        logger.warning(
                            f"{func.__name__} å¤±è´¥ ({attempt+1}/{max_retries}), "
                            f"{wait_time:.1f}såé‡è¯•: {e}"
                        )
                        time.sleep(wait_time)
                    else:
                        logger.error(
                            f"{func.__name__} å¤±è´¥å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}",
                            exc_info=True
                        )
            raise last_exception
        return wrapper
    return decorator
```

**æ­¥éª¤ 2**: åº”ç”¨åˆ°å…³é”®å‡½æ•°

ä¿®æ”¹ `services/trading-service/src/simple_scheduler.py`:

```python
from libs.common.error_handler import handle_database_errors, retry_on_failure

@handle_database_errors
def _query_kline_priority(top_n: int = 30) -> set:
    """Kçº¿ç»´åº¦ä¼˜å…ˆçº§"""
    # åŸæœ‰é€»è¾‘...
```

#### é¢„æœŸæ•ˆæœ

- é”™è¯¯ä¿¡æ¯æ›´è¯¦ç»†
- è‡ªåŠ¨é‡è¯•æœºåˆ¶

---

### é—®é¢˜ 10: ç¼ºå°‘å¥åº·æ£€æŸ¥ç«¯ç‚¹

**é—®é¢˜ ID**: P1-005
**ä¸¥é‡ç¨‹åº¦**: ğŸŸ¡ ä¸­
**å½±å“èŒƒå›´**: éƒ¨ç½²
**ä¿®å¤éš¾åº¦**: ä½

#### ä¿®å¤æ–¹æ¡ˆ

**æ­¥éª¤ 1**: ä¸ºæ¯ä¸ªæœåŠ¡æ·»åŠ å¥åº·æ£€æŸ¥

åˆ›å»º `services/trading-service/src/health.py`:

```python
"""å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
from fastapi import FastAPI
from fastapi.responses import JSONResponse
import psycopg
import sqlite3
from pathlib import Path

app = FastAPI()

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    checks = {
        "status": "healthy",
        "services": {}
    }

    # æ£€æŸ¥ TimescaleDB è¿æ¥
    try:
        with psycopg.connect(DB_URL) as conn:
            conn.execute("SELECT 1")
        checks["services"]["timescaledb"] = "ok"
    except Exception as e:
        checks["services"]["timescaledb"] = f"error: {e}"
        checks["status"] = "degraded"

    # æ£€æŸ¥ SQLite è¿æ¥
    try:
        conn = sqlite3.connect(SQLITE_PATH)
        conn.execute("SELECT 1")
        conn.close()
        checks["services"]["sqlite"] = "ok"
    except Exception as e:
        checks["services"]["sqlite"] = f"error: {e}"
        checks["status"] = "degraded"

    return JSONResponse(content=checks)


@app.get("/health/ready")
async def readiness_check():
    """å°±ç»ªæ£€æŸ¥"""
    # æ·»åŠ å°±ç»ªé€»è¾‘ï¼ˆå¦‚ï¼šæ•°æ®å·²åŠ è½½ã€æœåŠ¡å·²å¯åŠ¨ï¼‰
    return {"status": "ready"}
```

**æ­¥éª¤ 2**: æ›´æ–°å¯åŠ¨è„šæœ¬

ä¿®æ”¹ `services/trading-service/scripts/start.sh`:

```bash
start() {
    # ... åŸæœ‰å¯åŠ¨é€»è¾‘ ...

    # å¯åŠ¨å¥åº·æ£€æŸ¥æœåŠ¡ï¼ˆå¯é€‰ï¼‰
    if [ "$ENABLE_HEALTH_CHECK" = "true" ]; then
        python3 -m uvicorn src.health:app --host 0.0.0.0 --port 8080 &
        echo $! > $PID_DIR/health.pid
    fi
}

stop() {
    # ... åœæ­¢é€»è¾‘ ...

    if [ -f "$PID_DIR/health.pid" ]; then
        kill $(cat "$PID_DIR/health.pid")
        rm "$PID_DIR/health.pid"
    fi
}
```

**æ­¥éª¤ 3**: æ·»åŠ  Docker å¥åº·æ£€æŸ¥

åˆ›å»º `docker-compose.yml`:

```yaml
version: '3.8'

services:
  trading-service:
    build: ./services/trading-service
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
```

#### é¢„æœŸæ•ˆæœ

- æ”¯æŒ Docker healthcheck
- æ”¯æŒ Kubernetes liveness/readiness probe

---

## P2 - ä½ä¼˜å…ˆçº§ä¼˜åŒ–

### é—®é¢˜ 11: ç¡¬ç¼–ç å¸ç§è¿‡æ»¤åˆ—è¡¨

**é—®é¢˜ ID**: P2-001
**ä¸¥é‡ç¨‹åº¦**: ğŸŸ¢ ä½
**å½±å“èŒƒå›´**: å¯ç»´æŠ¤æ€§
**ä¿®å¤éš¾åº¦**: ä½

#### ä¿®å¤æ–¹æ¡ˆ

**æ­¥éª¤ 1**: ç§»è‡³é…ç½®æ–‡ä»¶

ä¿®æ”¹ `services/telegram-service/config/.env.example`:

```diff
+ # ---------- æ•°æ®è¿‡æ»¤ ----------
+ # ç¦æ­¢æ˜¾ç¤ºçš„å¸ç§ï¼ˆé€—å·åˆ†éš”ï¼‰
+ BLOCKED_SYMBOLS=BNXUSDT,ALPACAUSDT
```

**æ­¥éª¤ 2**: æ›´æ–°ä»£ç 

ä¿®æ”¹ `services/telegram-service/src/bot/app.py`:

```python
class UserRequestHandler:
    def __init__(self, card_registry: Optional[RankingRegistry] = None):
        # ä»é…ç½®è¯»å–
        self.blocked_symbols = set(
            os.getenv("BLOCKED_SYMBOLS", "BNXUSDT,ALPACAUSDT")
            .split(",")
        )
```

---

### é—®é¢˜ 12: æ·»åŠ å•å…ƒæµ‹è¯•

**é—®é¢˜ ID**: P2-002
**ä¸¥é‡ç¨‹åº¦**: ğŸŸ¢ ä½
**å½±å“èŒƒå›´**: ä»£ç è´¨é‡
**ä¿®å¤éš¾åº¦**: ä¸­

#### ä¿®å¤æ–¹æ¡ˆ

**æ­¥éª¤ 1**: åˆ›å»ºæµ‹è¯•æ¡†æ¶

```bash
# å®‰è£…æµ‹è¯•ä¾èµ–
pip install pytest pytest-asyncio pytest-cov

# åˆ›å»ºæµ‹è¯•ç›®å½•
mkdir -p services/trading-service/tests
```

**æ­¥éª¤ 2**: ç¼–å†™ç¤ºä¾‹æµ‹è¯•

åˆ›å»º `services/trading-service/tests/test_simple_scheduler.py`:

```python
"""æµ‹è¯• simple_scheduler"""
import pytest
from unittest.mock import Mock, patch
from datetime import datetime, timezone

@pytest.fixture
def mock_config():
    """æ¨¡æ‹Ÿé…ç½®"""
    import os
    os.environ["DATABASE_URL"] = "postgresql://test:test@localhost/test"
    os.environ["INDICATOR_SQLITE_PATH"] = "/tmp/test.db"


def test_parse_list():
    """æµ‹è¯•åˆ—è¡¨è§£æ"""
    from simple_scheduler import _parse_list

    assert _parse_list("BTCUSDT,ETHUSDT") == ["BTCUSDT", "ETHUSDT"]
    assert _parse_list("BTCUSDT, ETHUSDT,") == ["BTCUSDT", "ETHUSDT"]
    assert _parse_list("") == []


@pytest.mark.asyncio
async def test_get_high_priority_symbols_fast():
    """æµ‹è¯•é«˜ä¼˜å…ˆçº§å¸ç§è·å–"""
    from simple_scheduler import get_high_priority_symbols_fast

    with patch('simple_scheduler._query_kline_priority') as mock_kline:
        with patch('simple_scheduler._query_futures_priority') as mock_futures:
            mock_kline.return_value = {"BTCUSDT", "ETHUSDT"}
            mock_futures.return_value = {"ETHUSDT", "SOLUSDT"}

            result = get_high_priority_symbols_fast(top_n=10)

            assert "BTCUSDT" in result
            assert "ETHUSDT" in result
            assert "SOLUSDT" in result
```

**æ­¥éª¤ 3**: è¿è¡Œæµ‹è¯•

```bash
cd services/trading-service
pytest tests/ -v --cov=src
```

---

### é—®é¢˜ 13: å®šæœŸæ¸…ç† SQLite æ•°æ®åº“

**é—®é¢˜ ID**: P2-003
**ä¸¥é‡ç¨‹åº¦**: ğŸŸ¢ ä½
**å½±å“èŒƒå›´**: èµ„æºå ç”¨
**ä¿®å¤éš¾åº¦**: ä¸­

#### ä¿®å¤æ–¹æ¡ˆ

**æ­¥éª¤ 1**: åˆ›å»ºæ¸…ç†è„šæœ¬

æ–°å»º `scripts/cleanup_sqlite.sh`:

```bash
#!/bin/bash
# SQLite æ•°æ®åº“æ¸…ç†è„šæœ¬

DB_PATH="${1:-libs/database/services/telegram-service/market_data.db}"
KEEP_DAYS="${2:-30}"

echo "æ¸…ç† SQLite æ•°æ®åº“: $DB_PATH"
echo "ä¿ç•™æœ€è¿‘ $KEEP_DAYS å¤©æ•°æ®"

sqlite3 "$DB_PATH" << SQL
-- åˆ é™¤æ—§æ•°æ®
DELETE FROM [MACDæŸ±çŠ¶æ‰«æå™¨.py]
WHERE datetime(æ•°æ®æ—¶é—´) < datetime('now', '-$KEEP_DAYS days');

-- ä¼˜åŒ–æ•°æ®åº“
VACUUM;
SQL

echo "æ¸…ç†å®Œæˆ"
```

**æ­¥éª¤ 2**: æ·»åŠ åˆ° cron

```bash
# ç¼–è¾‘ crontab
crontab -e

# æ¯å‘¨æ—¥å‡Œæ™¨ 2 ç‚¹æ¸…ç†
0 2 * * 0 /home/lenovo/.projects/tradecat/scripts/cleanup_sqlite.sh
```

---

## æ¶æ„é‡æ„å»ºè®®

### å»ºè®® 1: å¼•å…¥æ¶ˆæ¯é˜Ÿåˆ—

**ç›®æ ‡**: è§£è€¦æœåŠ¡é—´é€šä¿¡

**æ–¹æ¡ˆ**:

1. é€‰æ‹©æ¶ˆæ¯é˜Ÿåˆ—: Redis Stream / RabbitMQ / Kafka
2. ä¿®æ”¹æ¶æ„:
   - data-service â†’ Redis Stream (K çº¿æ•°æ®)
   - trading-service â† Redis Stream (æ¶ˆè´¹æ•°æ®)
   - trading-service â†’ Redis Stream (æŒ‡æ ‡æ•°æ®)
   - telegram-service â† Redis Stream (æ¶ˆè´¹æŒ‡æ ‡)

**ç¤ºä¾‹**:

```python
# data-service/publisher.py
import redis

r = redis.Redis(host='localhost', port=6379, db=0)

def publish_kline(symbol: str, interval: str, data: dict):
    """å‘å¸ƒ K çº¿æ•°æ®"""
    r.xadd(f"kline:{interval}", {
        "symbol": symbol,
        "open": data['open'],
        "high": data['high'],
        "low": data['low'],
        "close": data['close'],
        "volume": data['volume']
    })


# trading-service/consumer.py
import redis

r = redis.Redis(host='localhost', port=6379, db=0)

def consume_klines(interval: str):
    """æ¶ˆè´¹ K çº¿æ•°æ®"""
    last_id = '$'
    while True:
        events = r.xread({f"kline:{interval}": last_id}, block=5000)
        if not events:
            continue

        for stream, messages in events:
            for msg_id, data in messages:
                process_kline(data)
                last_id = msg_id
```

---

### å»ºè®® 2: æ·»åŠ  API ç½‘å…³

**ç›®æ ‡**: ç»Ÿä¸€å…¥å£ï¼Œå¤„ç†è®¤è¯ã€é™æµ

**æ–¹æ¡ˆ**: ä½¿ç”¨ Kong / APISIX / Traefik

```yaml
# kong.yml
services:
  - name: telegram-service
    url: http://telegram-service:8080
    routes:
      - name: telegram-route
        paths:
          - /telegram
    plugins:
      - name: rate-limiting
        config:
          minute: 100
      - name: jwt
```

---

### å»ºè®® 3: ç›‘æ§å’Œå‘Šè­¦

**ç›®æ ‡**: å¯è§‚æµ‹æ€§

**æ–¹æ¡ˆ**:

1. **æŒ‡æ ‡é‡‡é›†**:
   - é›†æˆ Prometheus
   - æš´éœ² `/metrics` ç«¯ç‚¹

2. **æ—¥å¿—èšåˆ**:
   - ä½¿ç”¨ Loki / ELK

3. **é“¾è·¯è¿½è¸ª**:
   - é›†æˆ Jaeger / Zipkin

**ç¤ºä¾‹**:

```python
# metrics.py
from prometheus_client import Counter, Histogram, start_http_server

# å®šä¹‰æŒ‡æ ‡
kline_counter = Counter('klines_processed_total', 'Total klines processed', ['symbol'])
calc_duration = Histogram('indicator_calc_duration_seconds', 'Indicator calculation duration')

@calc_duration.time()
def calculate_indicator(symbol: str):
    """è®¡ç®—æŒ‡æ ‡"""
    kline_counter.labels(symbol=symbol).inc()
    # ... è®¡ç®—é€»è¾‘
```

---

## æ‰§è¡Œè®¡åˆ’

### é˜¶æ®µä¸€: é«˜ä¼˜å…ˆçº§ä¿®å¤ (1-2 å‘¨)

| ä»»åŠ¡ | è´Ÿè´£äºº | å·¥æœŸ | ä¾èµ– |
|:---|:---:|:---:|:---|
| P0-001: ç¡¬ç¼–ç è·¯å¾„ä¿®å¤ | Dev | 0.5 å¤© | - |
| P0-002: ä¾èµ–ç‰ˆæœ¬é”å®š | Dev | 1 å¤© | - |
| P0-003: ç¯å¢ƒå˜é‡ç»Ÿä¸€ | Dev | 0.5 å¤© | - |
| P0-004: æ•°æ®åº“ç´¢å¼•ä¼˜åŒ– | DBA | 2 å¤© | - |
| P0-005: SQLite å¹¶å‘ä¼˜åŒ– | Dev | 2 å¤© | P0-001 |

### é˜¶æ®µäºŒ: ä¸­ä¼˜å…ˆçº§ä¼˜åŒ– (2-3 å‘¨)

| ä»»åŠ¡ | è´Ÿè´£äºº | å·¥æœŸ | ä¾èµ– |
|:---|:---:|:---:|:---|
| P1-001: é…ç½®ä¸­å¿ƒå®ç° | Dev | 3 å¤© | é˜¶æ®µä¸€å®Œæˆ |
| P1-002: è™šæ‹Ÿç¯å¢ƒä¼˜åŒ– | Dev | 2 å¤© | - |
| P1-003: æ—¥å¿—ç»Ÿä¸€é…ç½® | Dev | 1 å¤© | - |
| P1-004: é”™è¯¯å¤„ç†æ”¹è¿› | Dev | 2 å¤© | - |
| P1-005: å¥åº·æ£€æŸ¥ç«¯ç‚¹ | Dev | 1 å¤© | - |

### é˜¶æ®µä¸‰: ä½ä¼˜å…ˆçº§ä¼˜åŒ– (1-2 å‘¨)

| ä»»åŠ¡ | è´Ÿè´£äºº | å·¥æœŸ | ä¾èµ– |
|:---|:---:|:---:|:---|
| P2-001: é…ç½®åŒ–ç¡¬ç¼–ç  | Dev | 0.5 å¤© | - |
| P2-002: å•å…ƒæµ‹è¯•æ·»åŠ  | Dev | 3 å¤© | - |
| P2-003: æ•°æ®æ¸…ç†è„šæœ¬ | Dev | 1 å¤© | - |

### é˜¶æ®µå››: æ¶æ„é‡æ„ (1-2 ä¸ªæœˆ)

| ä»»åŠ¡ | è´Ÿè´£äºº | å·¥æœŸ | ä¾èµ– |
|:---|:---:|:---:|:---|
| å»ºè®® 1: æ¶ˆæ¯é˜Ÿåˆ—å¼•å…¥ | Arch | 2 å‘¨ | P1-001 |
| å»ºè®® 2: API ç½‘å…³éƒ¨ç½² | Ops | 1 å‘¨ | å»ºè®® 1 |
| å»ºè®® 3: ç›‘æ§ç³»ç»Ÿæ­å»º | Ops | 2 å‘¨ | - |

---

## éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶

- [ ] æ‰€æœ‰é…ç½®é¡¹æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–
- [ ] æ‰€æœ‰ä¾èµ–ç‰ˆæœ¬å·²é”å®š
- [ ] æ•°æ®åº“æŸ¥è¯¢å“åº”æ—¶é—´ < 1s
- [ ] æ—  SQLite é”™è¯¯æ—¥å¿—
- [ ] å¥åº·æ£€æŸ¥ç«¯ç‚¹å¯è®¿é—®

### æ€§èƒ½éªŒæ”¶

- [ ] ä¼˜å…ˆçº§æŸ¥è¯¢è€—æ—¶ < 500ms
- [ ] æŒ‡æ ‡è®¡ç®—ååé‡ > 100 symbols/min
- [ ] è™šæ‹Ÿç¯å¢ƒç£ç›˜å ç”¨ < 300MB

### ç¨³å®šæ€§éªŒæ”¶

- [ ] 7 å¤©æ— å´©æºƒ
- [ ] é”™è¯¯ç‡ < 0.1%
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 60%

---

## é£é™©è¯„ä¼°

| é£é™© | å½±å“ | æ¦‚ç‡ | åº”å¯¹æªæ–½ |
|:---|:---:|:---:|:---|
| ä¾èµ–å‡çº§å¯¼è‡´å…¼å®¹æ€§é—®é¢˜ | é«˜ | ä¸­ | å……åˆ†æµ‹è¯•ï¼Œä¿ç•™å›æ»šæ–¹æ¡ˆ |
| æ•°æ®åº“è¿ç§»å¤±è´¥ | é«˜ | ä½ | å…ˆåœ¨æµ‹è¯•ç¯å¢ƒéªŒè¯ |
| é…ç½®å˜æ›´å¯¼è‡´æœåŠ¡å¯åŠ¨å¤±è´¥ | ä¸­ | ä¸­ | æ·»åŠ é…ç½®æ ¡éªŒé€»è¾‘ |
| è™šæ‹Ÿç¯å¢ƒé‡æ„å½±å“å¼€å‘ | ä¸­ | é«˜ | æä¾›è¿ç§»æ–‡æ¡£å’Œå·¥å…· |

---

## é™„å½•

### A. ç›¸å…³æ–‡æ¡£

- [AGENTS.md](AGENTS.md) - AI Agent æ“ä½œæ‰‹å†Œ
- [README.md](README.md) - é¡¹ç›®æ–‡æ¡£
- [TODO.md](TODO.md) - å¾…åŠäº‹é¡¹

### B. å‘½ä»¤é€ŸæŸ¥

```bash
# æ•°æ®åº“è¿æ¥
PGPASSWORD=postgres psql -h localhost -p 5433 -U postgres -d market_data

# SQLite è¿æ¥
sqlite3 libs/database/services/telegram-service/market_data.db

# æœåŠ¡å¯åŠ¨
./scripts/start.sh daemon

# æœåŠ¡åœæ­¢
./scripts/start.sh daemon-stop

# æŸ¥çœ‹æ—¥å¿—
tail -f services/*/logs/*.log
```

### C. è”ç³»æ–¹å¼

- é—®é¢˜åé¦ˆ: GitHub Issues
- æŠ€æœ¯è®¨è®º: Telegram ç¾¤ @glue_coding

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-01-03
**å®¡æ ¸çŠ¶æ€**: å¾…å®¡æ ¸
